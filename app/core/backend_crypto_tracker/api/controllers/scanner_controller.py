# workers/scanner_worker.py
import asyncio
import logging
import os
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import aiofiles
from dataclasses import dataclass, field
from enum import Enum
from app.core.backend_crypto_tracker.utils.logger import get_logger
from app.core.backend_crypto_tracker.utils.exceptions import APIException, DatabaseException
from app.core.backend_crypto_tracker.config.scanner_config import scanner_config
from app.core.backend_crypto_tracker.scanner.token_analyzer import TokenAnalyzer
from app.core.backend_crypto_tracker.processor.database.models.manager import DatabaseManager
from app.core.backend_crypto_tracker.processor.database.models.scan_job import ScanJob, ScanStatus

logger = get_logger(__name__)

class ScanStatus(Enum):
    IDLE = "idle"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

@dataclass
class ScanConfig:
    max_market_cap: float = 5_000_000  # $5M
    max_tokens_per_scan: int = 100
    scan_interval_hours: int = 6
    min_score_for_alert: float = 75.0
    email_alerts: bool = False
    telegram_alerts: bool = False
    export_results: bool = True
    cleanup_old_data_days: int = 30

@dataclass
class AlertConfig:
    email_host: str = "smtp.gmail.com"
    email_port: int = 587
    email_user: str = ""
    email_password: str = ""
    email_recipients: List[str] = field(default_factory=list)
    telegram_bot_token: str = ""
    telegram_chat_id: str = ""

class TokenAlertManager:
    def __init__(self, alert_config: AlertConfig):
        self.config = alert_config
    
    async def send_email_alert(self, subject: str, body: str):
        """Sendet E-Mail-Benachrichtigungen"""
        if not self.config.email_user or not self.config.email_recipients:
            return
        
        try:
            import smtplib
            from email.mime.text import MIMEText
            from email.mime.multipart import MIMEMultipart
            
            msg = MIMEMultipart()
            msg['From'] = self.config.email_user
            msg['To'] = ", ".join(self.config.email_recipients)
            msg['Subject'] = subject
            msg.attach(MIMEText(body, 'html'))
            
            with smtplib.SMTP(self.config.email_host, self.config.email_port) as server:
                server.starttls()
                server.login(self.config.email_user, self.config.email_password)
                server.send_message(msg)
            
            logger.info(f"Email alert sent: {subject}")
        except Exception as e:
            logger.error(f"Error sending email: {e}")
    
    async def send_telegram_alert(self, message: str):
        """Sendet Telegram-Benachrichtigungen"""
        if not self.config.telegram_bot_token or not self.config.telegram_chat_id:
            return
        
        try:
            import aiohttp
            
            url = f"https://api.telegram.org/bot{self.config.telegram_bot_token}/sendMessage"
            payload = {
                'chat_id': self.config.telegram_chat_id,
                'text': message,
                'parse_mode': 'HTML'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=payload) as response:
                    if response.status == 200:
                        logger.info("Telegram alert sent")
                    else:
                        logger.error(f"Telegram API error: {response.status}")
        except Exception as e:
            logger.error(f"Error sending Telegram message: {e}")
    
    def format_token_alert(self, tokens: List[Dict]) -> str:
        """Formatiert Token-Informationen fÃ¼r Alerts"""
        html_body = """
        <html>
        <body>
        <h2>ðŸš¨ High-Score Low-Cap Tokens Detected!</h2>
        <p>The following tokens have reached a high score:</p>
        <table border="1" style="border-collapse: collapse;">
        <tr style="background-color: #f2f2f2;">
            <th>Symbol</th>
            <th>Name</th>
            <th>Score</th>
            <th>Market Cap</th>
            <th>Chain</th>
        </tr>
        """
        
        for token in tokens:
            symbol = token.get('symbol', 'N/A')
            name = token.get('name', 'N/A')
            score = token.get('token_score', 0)
            market_cap = token.get('market_cap', 0)
            chain = token.get('chain', 'N/A')
            
            html_body += f"""
            <tr>
                <td>{symbol}</td>
                <td>{name}</td>
                <td style="color: green; font-weight: bold;">{score:.1f}</td>
                <td>${market_cap:,.0f}</td>
                <td>{chain}</td>
            </tr>
            """
        
        html_body += """
        </table>
        <p><small>Generated by Low-Cap Token Analyzer</small></p>
        </body>
        </html>
        """
        return html_body
    
    async def check_and_send_alerts(self, scan_results: List[Dict], min_score: float):
        """PrÃ¼ft Scan-Ergebnisse und sendet Alerts"""
        high_score_tokens = [
            result for result in scan_results
            if result.get('token_score', 0) >= min_score
        ]
        
        if not high_score_tokens:
            return
        
        # E-Mail Alert
        if self.config.email_recipients and self.config.email_user:
            subject = f"ðŸš¨ {len(high_score_tokens)} High-Score Tokens Found!"
            body = self.format_token_alert(high_score_tokens)
            await self.send_email_alert(subject, body)
        
        # Telegram Alert
        if self.config.telegram_bot_token and self.config.telegram_chat_id:
            message = f"ðŸš¨ <b>{len(high_score_tokens)} High-Score Tokens Found!</b>\n"
            for token in high_score_tokens[:5]:  # Max 5 fÃ¼r Telegram
                symbol = token.get('symbol', 'N/A')
                score = token.get('token_score', 0)
                message += f"â€¢ <b>{symbol}</b> - Score: {score:.1f}\n"
            
            if len(high_score_tokens) > 5:
                message += f"\n... and {len(high_score_tokens) - 5} more"
            
            await self.send_telegram_alert(message)

class ScanJobManager:
    def __init__(self, scan_config: ScanConfig, alert_config: AlertConfig):
        self.config = scan_config
        self.alert_manager = TokenAlertManager(alert_config)
        self.db_manager = DatabaseManager()
        self.token_analyzer = TokenAnalyzer()
        self.status = ScanStatus.IDLE
        self.last_scan_time = None
        self.scan_stats = {
            'total_scans': 0,
            'successful_scans': 0,
            'failed_scans': 0,
            'tokens_processed': 0,
            'high_score_tokens_found': 0
        }
    
    async def run_scan_job(self) -> Dict[str, Any]:
        """FÃ¼hrt einen kompletten Scan-Job aus"""
        if self.status == ScanStatus.RUNNING:
            logger.warning("Scan already running, skipping...")
            return {'status': 'skipped', 'reason': 'already_running'}
        
        self.status = ScanStatus.RUNNING
        start_time = datetime.now()
        job_result = {'status': 'unknown'}
        
        try:
            logger.info("Starting automated scan job...")
            self.scan_stats['total_scans'] += 1
            
            # FÃ¼hre Scan durch
            scan_results = await self.token_analyzer.scan_low_cap_tokens(
                max_tokens=self.config.max_tokens_per_scan,
                max_market_cap=self.config.max_market_cap
            )
            
            if not scan_results:
                logger.warning("No scan results received")
                self.scan_stats['failed_scans'] += 1
                job_result = {'status': 'failed', 'reason': 'no_results'}
                return job_result
            
            # Speichere Ergebnisse in Datenbank
            saved_count = 0
            for result in scan_results:
                try:
                    await self.db_manager.save_token_analysis(result)
                    saved_count += 1
                except Exception as e:
                    token_symbol = result.get('token_data', {}).get('symbol', 'Unknown')
                    logger.error(f"Error saving {token_symbol}: {e}")
            
            # Statistiken aktualisieren
            self.scan_stats['successful_scans'] += 1
            self.scan_stats['tokens_processed'] += len(scan_results)
            
            # High-Score Tokens identifizieren
            high_score_tokens = [
                result for result in scan_results
                if result.get('token_score', 0) >= self.config.min_score_for_alert
            ]
            self.scan_stats['high_score_tokens_found'] += len(high_score_tokens)
            
            # Alerts senden
            if high_score_tokens and (self.config.email_alerts or self.config.telegram_alerts):
                await self.alert_manager.check_and_send_alerts(
                    high_score_tokens,
                    self.config.min_score_for_alert
                )
            
            # Exportiere Ergebnisse
            if self.config.export_results:
                await self.export_scan_results(scan_results)
            
            # Bereinige alte Daten
            if self.config.cleanup_old_data_days > 0:
                await self.cleanup_old_data()
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            self.last_scan_time = end_time
            self.status = ScanStatus.COMPLETED
            
            job_result = {
                'status': 'success',
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'duration_seconds': duration,
                'tokens_scanned': len(scan_results),
                'tokens_saved': saved_count,
                'high_score_tokens': len(high_score_tokens),
                'scan_stats': self.scan_stats.copy()
            }
            
            logger.info(f"Scan job completed: {len(scan_results)} tokens processed, "
                       f"{len(high_score_tokens)} high-score tokens found")
        except Exception as e:
            logger.error(f"Error during scan job: {e}")
            self.scan_stats['failed_scans'] += 1
            self.status = ScanStatus.FAILED
            
            job_result = {
                'status': 'error',
                'error': str(e),
                'start_time': start_time.isoformat() if start_time else None,
                'end_time': datetime.now().isoformat()
            }
        
        return job_result
    
    async def export_scan_results(self, scan_results: List[Dict]):
        """Exportiert Scan-Ergebnisse"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            json_filename = f"exports/scan_results_{timestamp}.json"
            
            # Ensure exports directory exists
            os.makedirs(os.path.dirname(json_filename), exist_ok=True)
            
            export_data = []
            for result in scan_results:
                token_data = result.get('token_data', {})
                export_data.append({
                    'token': {
                        'address': token_data.get('address', ''),
                        'name': token_data.get('name', ''),
                        'symbol': token_data.get('symbol', '').upper(),
                        'chain': token_data.get('chain', ''),
                        'market_cap': token_data.get('market_cap', 0),
                        'volume_24h': token_data.get('volume_24h', 0)
                    },
                    'score': result.get('token_score', 0),
                    'metrics': result.get('metrics', {}),
                    'analysis_date': result.get('analysis_date', datetime.min).isoformat()
                })
            
            async with aiofiles.open(json_filename, 'w') as f:
                await f.write(json.dumps(export_data, indent=2, default=str))
            
            logger.info(f"Scan results exported to: {json_filename}")
        except Exception as e:
            logger.error(f"Error exporting results: {e}")
    
    async def cleanup_old_data(self):
        """Bereinigt alte Daten aus der Datenbank"""
        try:
            cutoff_date = datetime.now() - timedelta(days=self.config.cleanup_old_data_days)
            await self.db_manager.cleanup_old_data(cutoff_date)
            logger.info(f"Cleaned up data older than {cutoff_date}")
        except Exception as e:
            logger.error(f"Error cleaning up data: {e}")
    
    def get_status(self) -> Dict[str, Any]:
        """Gibt den aktuellen Status zurÃ¼ck"""
        return {
            'status': self.status.value,
            'last_scan_time': self.last_scan_time.isoformat() if self.last_scan_time else None,
            'scan_stats': self.scan_stats.copy(),
            'config': {
                'scan_interval_hours': self.config.scan_interval_hours,
                'max_tokens_per_scan': self.config.max_tokens_per_scan,
                'min_score_for_alert': self.config.min_score_for_alert
            }
        }

class ScannerWorker:
    """Worker-Klasse fÃ¼r Scanner-Operationen, die von ScannerController verwendet wird"""
    
    def __init__(self, scan_config: ScanConfig = None, alert_config: AlertConfig = None):
        # Standardkonfiguration verwenden, wenn keine angegeben
        self.scan_config = scan_config or ScanConfig()
        self.alert_config = alert_config or AlertConfig()
        
        # JobManager initialisieren
        self.job_manager = ScanJobManager(self.scan_config, self.alert_config)
        
        # Token Analyzer und Database Manager fÃ¼r direkten Zugriff
        self.token_analyzer = TokenAnalyzer()
        self.db_manager = DatabaseManager()
        
        # Aktive Scans speichern
        self.active_scans = {}
        
        logger.info("ScannerWorker initialized")
    
    async def start_discovery_scan(self, chains: List[str], max_market_cap: float, max_tokens: int) -> str:
        """Startet einen Discovery-Scan und gibt die Scan-ID zurÃ¼ck"""
        scan_id = f"discovery_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
        
        # Neuen Scan-Job erstellen
        scan_job = ScanJob(
            id=scan_id,
            status=ScanStatus.SCANNING,
            progress=0.0,
            start_time=datetime.utcnow(),
            chain=",".join(chains),
            scan_type="discovery"
        )
        
        self.active_scans[scan_id] = scan_job
        
        # Scan im Hintergrund starten
        asyncio.create_task(self._run_discovery_scan(scan_id, chains, max_market_cap, max_tokens))
        
        logger.info(f"Started discovery scan {scan_id} for chains: {chains}")
        return scan_id
    
    async def start_analysis_scan(self, token_addresses: List[str], chain: str, include_advanced_metrics: bool) -> str:
        """Startet einen Analyse-Scan und gibt die Scan-ID zurÃ¼ck"""
        scan_id = f"analysis_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
        
        # Neuen Scan-Job erstellen
        scan_job = ScanJob(
            id=scan_id,
            status=ScanStatus.ANALYZING,
            progress=0.0,
            start_time=datetime.utcnow(),
            chain=chain,
            scan_type="analysis"
        )
        
        self.active_scans[scan_id] = scan_job
        
        # Analyse im Hintergrund starten
        asyncio.create_task(self._run_analysis_scan(scan_id, token_addresses, chain, include_advanced_metrics))
        
        logger.info(f"Started analysis scan {scan_id} for {len(token_addresses)} tokens on {chain}")
        return scan_id
    
    async def stop_scan(self, scan_id: str) -> bool:
        """Stoppt einen aktiven Scan"""
        if scan_id not in self.active_scans:
            logger.warning(f"Scan {scan_id} not found or not active")
            return False
        
        scan_job = self.active_scans[scan_id]
        scan_job.status = ScanStatus.STOPPED
        scan_job.end_time = datetime.utcnow()
        
        # Zum Verlauf hinzufÃ¼gen und aus aktiven Scans entfernen
        self.active_scans.pop(scan_id, None)
        
        logger.info(f"Stopped scan {scan_id}")
        return True
    
    def get_scan_status(self, scan_id: str) -> Optional[Dict[str, Any]]:
        """Gibt den Status eines Scans zurÃ¼ck"""
        if scan_id in self.active_scans:
            scan_job = self.active_scans[scan_id]
            return {
                "scan_id": scan_id,
                "status": scan_job.status.value,
                "progress": scan_job.progress,
                "start_time": scan_job.start_time.isoformat(),
                "chain": scan_job.chain,
                "scan_type": scan_job.scan_type,
                "tokens_found": scan_job.tokens_found,
                "tokens_analyzed": scan_job.tokens_analyzed,
                "high_risk_tokens": scan_job.high_risk_tokens
            }
        return None
    
    async def _run_discovery_scan(self, scan_id: str, chains: List[str], max_market_cap: float, max_tokens: int):
        """FÃ¼hrt einen Discovery-Scan im Hintergrund durch"""
        try:
            scan_job = self.active_scans.get(scan_id)
            if not scan_job:
                logger.error(f"Scan job {scan_id} not found")
                return
            
            tokens_found = 0
            high_risk_tokens = 0
            
            try:
                # Token Discovery durchfÃ¼hren
                for chain in chains:
                    tokens = await self.token_analyzer.discover_tokens(
                        chain, max_market_cap, max_tokens // len(chains)
                    )
                    
                    tokens_found += len(tokens)
                    
                    # Scan-Fortschritt aktualisieren
                    if scan_id in self.active_scans:
                        scan_job.tokens_found = tokens_found
                        scan_job.progress = min(0.9, tokens_found / max_tokens)
                    
                    # Tokens analysieren
                    for token in tokens:
                        try:
                            analysis = await self.token_analyzer.analyze_token(token)
                            
                            if analysis.get('token_score', 0) < 30:  # Niedriger Score = hohes Risiko
                                high_risk_tokens += 1
                        except Exception as e:
                            logger.warning(f"Error analyzing token {token.symbol}: {e}")
                
                # Scan abschlieÃŸen
                if scan_id in self.active_scans:
                    scan_job.status = ScanStatus.COMPLETED
                    scan_job.end_time = datetime.utcnow()
                    scan_job.tokens_analyzed = tokens_found
                    scan_job.high_risk_tokens = high_risk_tokens
                    scan_job.progress = 1.0
                    
                    # Aus aktiven Scans entfernen
                    self.active_scans.pop(scan_id, None)
                    
                    logger.info(f"Discovery scan {scan_id} completed: {tokens_found} tokens found, {high_risk_tokens} high risk")
                
            except Exception as e:
                logger.error(f"Error in discovery scan {scan_id}: {e}")
                
                if scan_id in self.active_scans:
                    scan_job.status = ScanStatus.FAILED
                    scan_job.end_time = datetime.utcnow()
                    scan_job.error_message = str(e)
                    
                    # Aus aktiven Scans entfernen
                    self.active_scans.pop(scan_id, None)
        
        except Exception as e:
            logger.error(f"Critical error in discovery scan {scan_id}: {e}")
            
            if scan_id in self.active_scans:
                scan_job = self.active_scans[scan_id]
                scan_job.status = ScanStatus.FAILED
                scan_job.end_time = datetime.utcnow()
                scan_job.error_message = str(e)
                
                # Aus aktiven Scans entfernen
                self.active_scans.pop(scan_id, None)
    
    async def _run_analysis_scan(self, scan_id: str, token_addresses: List[str], chain: str, include_advanced_metrics: bool):
        """FÃ¼hrt einen Analyse-Scan im Hintergrund durch"""
        try:
            scan_job = self.active_scans.get(scan_id)
            if not scan_job:
                logger.error(f"Scan job {scan_id} not found")
                return
            
            tokens_analyzed = 0
            high_risk_tokens = 0
            
            try:
                # Token-Analysen durchfÃ¼hren
                for i, token_address in enumerate(token_addresses):
                    try:
                        analysis = await self.token_analyzer.analyze_custom_token(token_address, chain)
                        
                        tokens_analyzed += 1
                        
                        # Fortschritt aktualisieren
                        if scan_id in self.active_scans:
                            scan_job.tokens_analyzed = tokens_analyzed
                            scan_job.progress = min(0.9, tokens_analyzed / len(token_addresses))
                        
                        # Risiko bewerten
                        score = analysis.get('score', 50)
                        if include_advanced_metrics:
                            try:
                                # Erweiterte Analyse durchfÃ¼hren
                                wallet_analyses = analysis.get('wallet_analyses', {}).get('top_holders', [])
                                
                                # Erweiterte Scoring-Engine nutzen
                                from app.core.backend_crypto_tracker.scanner.scoring_engine import MultiChainScoringEngine
                                scoring_engine = MultiChainScoringEngine()
                                advanced_score = await scoring_engine.calculate_token_score_advanced(
                                    analysis.get('token_info', {}),
                                    wallet_analyses,
                                    chain
                                )
                                
                                if advanced_score.get('institutional_score', 50) < 30:
                                    high_risk_tokens += 1
                            except Exception as e:
                                logger.warning(f"Error in advanced scoring for {token_address}: {e}")
                        else:
                            if score < 30:
                                high_risk_tokens += 1
                    except Exception as e:
                        logger.warning(f"Error analyzing token {token_address}: {e}")
                
                # Scan abschlieÃŸen
                if scan_id in self.active_scans:
                    scan_job.status = ScanStatus.COMPLETED
                    scan_job.end_time = datetime.utcnow()
                    scan_job.tokens_found = tokens_analyzed
                    scan_job.high_risk_tokens = high_risk_tokens
                    scan_job.progress = 1.0
                    
                    # Aus aktiven Scans entfernen
                    self.active_scans.pop(scan_id, None)
                    
                    logger.info(f"Analysis scan {scan_id} completed: {tokens_analyzed} tokens analyzed, {high_risk_tokens} high risk")
                
            except Exception as e:
                logger.error(f"Error in analysis scan {scan_id}: {e}")
                
                if scan_id in self.active_scans:
                    scan_job.status = ScanStatus.FAILED
                    scan_job.end_time = datetime.utcnow()
                    scan_job.error_message = str(e)
                    
                    # Aus aktiven Scans entfernen
                    self.active_scans.pop(scan_id, None)
        
        except Exception as e:
            logger.error(f"Critical error in analysis scan {scan_id}: {e}")
            
            if scan_id in self.active_scans:
                scan_job = self.active_scans[scan_id]
                scan_job.status = ScanStatus.FAILED
                scan_job.end_time = datetime.utcnow()
                scan_job.error_message = str(e)
                
                # Aus aktiven Scans entfernen
                self.active_scans.pop(scan_id, None)
    
    def get_active_scans(self) -> Dict[str, Any]:
        """Gibt alle aktiven Scans zurÃ¼ck"""
        return {
            scan_id: {
                "status": scan.status.value,
                "progress": scan.progress,
                "start_time": scan.start_time.isoformat(),
                "chain": scan.chain,
                "scan_type": scan.scan_type,
                "tokens_found": scan.tokens_found,
                "tokens_analyzed": scan.tokens_analyzed,
                "high_risk_tokens": scan.high_risk_tokens
            }
            for scan_id, scan in self.active_scans.items()
        }
    
    def get_worker_status(self) -> Dict[str, Any]:
        """Gibt den Status des Workers zurÃ¼ck"""
        return {
            "active_scans_count": len(self.active_scans),
            "job_manager_status": self.job_manager.get_status(),
            "last_updated": datetime.utcnow().isoformat()
        }

async-Verwendung
async def main():
    # Konfiguration laden
    scan_config = ScanConfig(
        max_market_cap=5_000_000,
        max_tokens_per_scan=50,
        scan_interval_hours=6,
        min_score_for_alert=75.0,
        email_alerts=True,
        telegram_alerts=True,
        export_results=True,
        cleanup_old_data_days=30
    )
    
    alert_config = AlertConfig(
        email_user=os.getenv("EMAIL_USER"),
        email_password=os.getenv("EMAIL_PASSWORD"),
        email_recipients=[os.getenv("EMAIL_RECIPIENT")],
        telegram_bot_token=os.getenv("TELEGRAM_BOT_TOKEN"),
        telegram_chat_id=os.getenv("TELEGRAM_CHAT_ID")
    )
    
    job_manager = ScanJobManager(scan_config, alert_config)
    result = await job_manager.run_scan_job()
    print(json.dumps(result, indent=2))

if __name__ == "__main__":
    asyncio.run(main())
